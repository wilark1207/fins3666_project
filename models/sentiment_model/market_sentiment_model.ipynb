{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "9898c8eb",
      "metadata": {},
      "source": [
        "# Market Sentiment Model\n",
        "End-to-end notebook that builds a daily sentiment index from provided headlines, aligns it with price data, and produces trade signals ready to combine with Jake and Malcolm's strategy.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "63d32a6d",
      "metadata": {},
      "source": [
        "## 1) Imports\n",
        "Using pandas_datareader for market data and VADER with a small finance lexicon extension for sentiment.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "id": "5995b524",
      "metadata": {},
      "outputs": [],
      "source": [
        "import json\n",
        "from pathlib import Path\n",
        "from datetime import datetime\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import yfinance as yf\n",
        "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
        "\n",
        "pd.options.display.float_format = \"{:.4f}\".format\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2a8120ba",
      "metadata": {},
      "source": [
        "## 2) Config\n",
        "Adjust ticker, dates, thresholds, and paths as needed. News is loaded from the provided CSV in `Data/news_headlines.csv`.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "id": "59114f91",
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "# Core parameters\n",
        "TICKER = \"SPY\"  # proxy for broad US market\n",
        "START_DATE = \"2008-01-01\"\n",
        "END_DATE = \"2024-12-31\"\n",
        "\n",
        "# Sentiment controls\n",
        "POS_THRESHOLD = 0.20\n",
        "NEG_THRESHOLD = -0.20\n",
        "MIN_HEADLINES = 3\n",
        "ROLLING_Z_WINDOW = 30  # trading days\n",
        "\n",
        "# Data locations\n",
        "NEWS_PATH = Path(\"../../Data/news_headlines.csv\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b9dee71a",
      "metadata": {},
      "source": [
        "## 3) Price data via yfinance (with optional CSV cache)\n",
        "Uses yfinance to avoid pandas_datareader/distutils issues. If network blocked, point to a cached CSV with date/close.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7ec7a5e0",
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/var/folders/hp/kfy3hmy15mn6824slvj7zgd40000gn/T/ipykernel_64658/1999575017.py:10: FutureWarning: YF.download() has changed argument auto_adjust default to True\n",
            "  px = yf.download(ticker, start=start_dt, end=end_dt, progress=False)\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th>Price</th>\n",
              "      <th>date</th>\n",
              "      <th>close</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>4273</th>\n",
              "      <td>2024-12-23</td>\n",
              "      <td>589.5244</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4274</th>\n",
              "      <td>2024-12-24</td>\n",
              "      <td>596.0769</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4275</th>\n",
              "      <td>2024-12-26</td>\n",
              "      <td>596.1167</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4276</th>\n",
              "      <td>2024-12-27</td>\n",
              "      <td>589.8416</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4277</th>\n",
              "      <td>2024-12-30</td>\n",
              "      <td>583.1106</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "Price       date    close\n",
              "4273  2024-12-23 589.5244\n",
              "4274  2024-12-24 596.0769\n",
              "4275  2024-12-26 596.1167\n",
              "4276  2024-12-27 589.8416\n",
              "4277  2024-12-30 583.1106"
            ]
          },
          "execution_count": 64,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "\n",
        "\n",
        "def fetch_prices(ticker: str, start: str, end: str, cached_csv: Path | None = None) -> pd.DataFrame:\n",
        "    if cached_csv is not None and cached_csv.exists():\n",
        "        px = pd.read_csv(cached_csv, parse_dates=[\"date\"])\n",
        "        if \"close\" not in px.columns:\n",
        "            raise ValueError(\"Cached price file must include 'date' and 'close' columns.\")\n",
        "        return px[[\"date\", \"close\"]].sort_values(\"date\").reset_index(drop=True)\n",
        "\n",
        "    start_dt = pd.to_datetime(start)\n",
        "    end_dt = pd.to_datetime(end)\n",
        "    px = yf.download(ticker, start=start_dt, end=end_dt, progress=False)\n",
        "\n",
        "    if px.empty:\n",
        "        raise ValueError(\"yfinance returned no data. Provide cached_csv with date/close columns.\")\n",
        "\n",
        "    # If MultiIndex columns (e.g. fields \u00d7 tickers), select this ticker\n",
        "    if isinstance(px.columns, pd.MultiIndex):\n",
        "        # typical yfinance layout: level 0 = field, level 1 = ticker\n",
        "        if ticker in px.columns.get_level_values(1):\n",
        "            px = px.xs(ticker, axis=1, level=1)\n",
        "        else:\n",
        "            # fallback: just take the first ticker if something is weird\n",
        "            px = px.xs(px.columns.levels[1][0], axis=1, level=1)\n",
        "\n",
        "    px = px.rename(columns=str.lower)\n",
        "\n",
        "    # Populate close from available candidates\n",
        "    if \"close\" not in px.columns and \"adj close\" in px.columns:\n",
        "        px[\"close\"] = px[\"adj close\"]\n",
        "\n",
        "    if \"close\" not in px.columns:\n",
        "        # Try capitalized variants before giving up\n",
        "        for cand in [\"Close\", \"Adj Close\"]:\n",
        "            if cand in px.columns:\n",
        "                px[\"close\"] = px[cand]\n",
        "                break\n",
        "\n",
        "    if \"close\" not in px.columns:\n",
        "        raise ValueError(f\"Price data missing 'close' column after download. Columns: {list(px.columns)}\")\n",
        "\n",
        "    prices = px[[\"close\"]].reset_index().rename(columns={\"Date\": \"date\", \"index\": \"date\"})\n",
        "    prices[\"date\"] = pd.to_datetime(prices[\"date\"])\n",
        "    return prices\n",
        "\n",
        "prices = fetch_prices(TICKER, START_DATE, END_DATE)\n",
        "prices.tail()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b19b60eb",
      "metadata": {},
      "source": [
        "## 4) Load headline data\n",
        "The CSV from `Data/news_headlines.csv` is expected to have at least columns `Title` and `Date`. Any `CP`/price column is kept for reference but not required.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "id": "ed43dbcb",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>headline</th>\n",
              "      <th>date</th>\n",
              "      <th>CP</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>JPMorgan Predicts 2008 Will Be \"Nothing But Net\"</td>\n",
              "      <td>2008-01-02</td>\n",
              "      <td>1447.1600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Dow Tallies Biggest First-session-of-year Poin...</td>\n",
              "      <td>2008-01-02</td>\n",
              "      <td>1447.1600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2008 predictions for the S&amp;P 500</td>\n",
              "      <td>2008-01-02</td>\n",
              "      <td>1447.1600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>U.S. Stocks Higher After Economic Data, Monsan...</td>\n",
              "      <td>2008-01-03</td>\n",
              "      <td>1447.1600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>U.S. Stocks Climb As Hopes Increase For More F...</td>\n",
              "      <td>2008-01-07</td>\n",
              "      <td>1416.1800</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                            headline       date        CP\n",
              "0   JPMorgan Predicts 2008 Will Be \"Nothing But Net\" 2008-01-02 1447.1600\n",
              "1  Dow Tallies Biggest First-session-of-year Poin... 2008-01-02 1447.1600\n",
              "2                   2008 predictions for the S&P 500 2008-01-02 1447.1600\n",
              "3  U.S. Stocks Higher After Economic Data, Monsan... 2008-01-03 1447.1600\n",
              "4  U.S. Stocks Climb As Hopes Increase For More F... 2008-01-07 1416.1800"
            ]
          },
          "execution_count": 65,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "\n",
        "def load_headlines(path: Path) -> pd.DataFrame:\n",
        "    df = pd.read_csv(path)\n",
        "    # Standardise column names\n",
        "    rename_map = {\"Title\": \"headline\", \"Date\": \"date\"}\n",
        "    df = df.rename(columns={k: v for k, v in rename_map.items() if k in df.columns})\n",
        "\n",
        "    if \"date\" not in df.columns or \"headline\" not in df.columns:\n",
        "        raise ValueError(\"Headline data must include 'Date' and 'Title' columns.\")\n",
        "\n",
        "    df[\"date\"] = pd.to_datetime(df[\"date\"])\n",
        "    df[\"headline\"] = df[\"headline\"].astype(str).str.strip()\n",
        "    df = df.dropna(subset=[\"date\", \"headline\"])\n",
        "    return df\n",
        "\n",
        "\n",
        "headlines = load_headlines(NEWS_PATH)\n",
        "headlines.head()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bfe0af64",
      "metadata": {},
      "source": [
        "## 5) Sentiment model with finance slang extension\n",
        "Extends VADER with a few retail/finance terms to improve polarity detection.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "id": "89fb3416",
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "def build_analyzer() -> SentimentIntensityAnalyzer:\n",
        "    sia = SentimentIntensityAnalyzer()\n",
        "    finance_lexicon = {\n",
        "        \"mooning\": 3.2,\n",
        "        \"rekt\": -3.4,\n",
        "        \"bagholder\": -2.4,\n",
        "        \"bagholders\": -2.4,\n",
        "        \"diamond hands\": 2.2,\n",
        "        \"paper hands\": -2.0,\n",
        "        \"buy the dip\": 1.8,\n",
        "        \"dead cat\": -2.1,\n",
        "        \"rocket\": 2.4,\n",
        "        \"to the moon\": 2.6,\n",
        "    }\n",
        "    sia.lexicon.update(finance_lexicon)\n",
        "    return sia\n",
        "\n",
        "\n",
        "sia = build_analyzer()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d4ce3644",
      "metadata": {},
      "source": [
        "## 6) Score headlines\n",
        "Compute VADER compound scores per headline.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "id": "b39e556a",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>headline</th>\n",
              "      <th>date</th>\n",
              "      <th>CP</th>\n",
              "      <th>compound</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>JPMorgan Predicts 2008 Will Be \"Nothing But Net\"</td>\n",
              "      <td>2008-01-02</td>\n",
              "      <td>1447.1600</td>\n",
              "      <td>0.0000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Dow Tallies Biggest First-session-of-year Poin...</td>\n",
              "      <td>2008-01-02</td>\n",
              "      <td>1447.1600</td>\n",
              "      <td>-0.2732</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2008 predictions for the S&amp;P 500</td>\n",
              "      <td>2008-01-02</td>\n",
              "      <td>1447.1600</td>\n",
              "      <td>0.0000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>U.S. Stocks Higher After Economic Data, Monsan...</td>\n",
              "      <td>2008-01-03</td>\n",
              "      <td>1447.1600</td>\n",
              "      <td>0.0000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>U.S. Stocks Climb As Hopes Increase For More F...</td>\n",
              "      <td>2008-01-07</td>\n",
              "      <td>1416.1800</td>\n",
              "      <td>0.6249</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                            headline       date        CP  \\\n",
              "0   JPMorgan Predicts 2008 Will Be \"Nothing But Net\" 2008-01-02 1447.1600   \n",
              "1  Dow Tallies Biggest First-session-of-year Poin... 2008-01-02 1447.1600   \n",
              "2                   2008 predictions for the S&P 500 2008-01-02 1447.1600   \n",
              "3  U.S. Stocks Higher After Economic Data, Monsan... 2008-01-03 1447.1600   \n",
              "4  U.S. Stocks Climb As Hopes Increase For More F... 2008-01-07 1416.1800   \n",
              "\n",
              "   compound  \n",
              "0    0.0000  \n",
              "1   -0.2732  \n",
              "2    0.0000  \n",
              "3    0.0000  \n",
              "4    0.6249  "
            ]
          },
          "execution_count": 67,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "\n",
        "def score_headlines(df: pd.DataFrame, analyzer: SentimentIntensityAnalyzer) -> pd.DataFrame:\n",
        "    scored = df.copy()\n",
        "    scored[\"compound\"] = scored[\"headline\"].apply(lambda txt: analyzer.polarity_scores(txt)[\"compound\"])\n",
        "    return scored\n",
        "\n",
        "\n",
        "scored_headlines = score_headlines(headlines, sia)\n",
        "scored_headlines.head()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d6f936b3",
      "metadata": {},
      "source": [
        "## 7) Daily aggregation\n",
        "Capture mean, extremes, count, and an optional rolling z-score for the mean sentiment.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 68,
      "id": "00163ed9",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "        date  compound_mean  compound_max  compound_min  headline_count  \\\n",
            "0 2008-01-02        -0.0911        0.0000       -0.2732               3   \n",
            "1 2008-01-03         0.0000        0.0000        0.0000               1   \n",
            "2 2008-01-07         0.6249        0.6249        0.6249               1   \n",
            "3 2008-01-09         0.2048        0.6597       -0.2500               2   \n",
            "4 2008-01-10         0.0000        0.0000        0.0000               1   \n",
            "\n",
            "                                            headline  compound_z  \n",
            "0  JPMorgan Predicts 2008 Will Be \"Nothing But Ne...         NaN  \n",
            "1  U.S. Stocks Higher After Economic Data, Monsan...         NaN  \n",
            "2  U.S. Stocks Climb As Hopes Increase For More F...         NaN  \n",
            "3  How Investing in Intangibles -- Like Employee ...         NaN  \n",
            "4  U.S. Stocks Zigzag Higher As Bernanke Speech S...     -0.5737  \n"
          ]
        }
      ],
      "source": [
        "\n",
        "def aggregate_daily(df: pd.DataFrame, window: int = ROLLING_Z_WINDOW) -> pd.DataFrame:\n",
        "    grouped = (\n",
        "        df.groupby(\"date\")[\"compound\"]\n",
        "        .agg([\"mean\", \"max\", \"min\", \"count\"])\n",
        "        .reset_index()\n",
        "        .rename(columns={\n",
        "            \"mean\": \"compound_mean\",\n",
        "            \"max\": \"compound_max\",\n",
        "            \"min\": \"compound_min\",\n",
        "            \"count\": \"headline_count\",\n",
        "        })\n",
        "    )\n",
        "\n",
        "    # Collect all headlines per day (single string, separated by ||)\n",
        "    headline_text = (\n",
        "        df.groupby(\"date\")[\"headline\"]\n",
        "        .apply(lambda s: \" || \".join(s.astype(str)))\n",
        "        .reset_index()\n",
        "    )\n",
        "    grouped = grouped.merge(headline_text, on=\"date\", how=\"left\")\n",
        "\n",
        "    def last_point_z(series: pd.Series) -> float:\n",
        "        if series.shape[0] < 5:\n",
        "            return np.nan\n",
        "        std = series.std(ddof=0)\n",
        "        if std == 0:\n",
        "            return 0.0\n",
        "        return (series.iloc[-1] - series.mean()) / std\n",
        "\n",
        "    grouped[\"compound_z\"] = grouped[\"compound_mean\"].rolling(window, min_periods=5).apply(last_point_z, raw=False)\n",
        "    return grouped\n",
        "\n",
        "\n",
        "daily_sentiment = aggregate_daily(scored_headlines)\n",
        "print(daily_sentiment.head())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "44297e0e",
      "metadata": {},
      "source": [
        "## 8) Signal construction\n",
        "Build a daily sentiment score that blends average and extremes, then convert to buy/sell/hold using thresholds. Filters out days with too few headlines or conflicted sentiment.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 69,
      "id": "da01a2f8",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "        date  compound_mean  compound_max  compound_min  headline_count  \\\n",
            "0 2008-01-02        -0.0911        0.0000       -0.2732               3   \n",
            "1 2008-01-03         0.0000        0.0000        0.0000               1   \n",
            "2 2008-01-07         0.6249        0.6249        0.6249               1   \n",
            "3 2008-01-09         0.2048        0.6597       -0.2500               2   \n",
            "4 2008-01-10         0.0000        0.0000        0.0000               1   \n",
            "\n",
            "                                            headline  compound_z  final_score  \\\n",
            "0  JPMorgan Predicts 2008 Will Be \"Nothing But Ne...         NaN      -0.1594   \n",
            "1  U.S. Stocks Higher After Economic Data, Monsan...         NaN       0.0000   \n",
            "2  U.S. Stocks Climb As Hopes Increase For More F...         NaN       0.9374   \n",
            "3  How Investing in Intangibles -- Like Employee ...         NaN       0.3073   \n",
            "4  U.S. Stocks Zigzag Higher As Bernanke Speech S...     -0.5737       0.0000   \n",
            "\n",
            "   too_few_headlines  conflicting sentiment_signal  \n",
            "0              False        False             hold  \n",
            "1               True        False             hold  \n",
            "2               True        False             hold  \n",
            "3               True         True             hold  \n",
            "4               True        False             hold  \n",
            "Saved daily sentiment with signals and headlines to headline_compound.csv\n"
          ]
        }
      ],
      "source": [
        "\n",
        "def construct_signals(df: pd.DataFrame) -> pd.DataFrame:\n",
        "    signals = df.copy()\n",
        "    signals[\"final_score\"] = (\n",
        "        signals[\"compound_mean\"]\n",
        "        + 0.25 * signals[\"compound_max\"]\n",
        "        + 0.25 * signals[\"compound_min\"]\n",
        "    )\n",
        "\n",
        "    signals[\"too_few_headlines\"] = signals[\"headline_count\"] < MIN_HEADLINES\n",
        "    signals[\"conflicting\"] = (signals[\"compound_max\"] > 0) & (signals[\"compound_min\"] < 0)\n",
        "\n",
        "    def classify(row):\n",
        "        if row[\"too_few_headlines\"] or row[\"conflicting\"]:\n",
        "            return \"hold\"\n",
        "        if row[\"final_score\"] >= POS_THRESHOLD:\n",
        "            return \"buy\"\n",
        "        if row[\"final_score\"] <= NEG_THRESHOLD:\n",
        "            return \"sell\"\n",
        "        return \"hold\"\n",
        "\n",
        "    signals[\"sentiment_signal\"] = signals.apply(classify, axis=1)\n",
        "    return signals\n",
        "\n",
        "\n",
        "sentiment_signals = construct_signals(daily_sentiment)\n",
        "print(sentiment_signals.head())\n",
        "\n",
        "out_path = Path(\"headline_compound.csv\")\n",
        "sentiment_signals.to_csv(out_path, index=False)\n",
        "print(f\"Saved daily sentiment with signals and headlines to {out_path}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9e3ccdaa",
      "metadata": {},
      "source": [
        "## 9) Align sentiment with SPY returns (no look-ahead)\n",
        "Merge daily sentiment with SPY prices, compute returns, and shift signals/scores by one day for validation.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 71,
      "id": "a6b2c2c7",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>date</th>\n",
              "      <th>close</th>\n",
              "      <th>compound_mean</th>\n",
              "      <th>compound_max</th>\n",
              "      <th>compound_min</th>\n",
              "      <th>headline_count</th>\n",
              "      <th>headline</th>\n",
              "      <th>compound_z</th>\n",
              "      <th>final_score</th>\n",
              "      <th>too_few_headlines</th>\n",
              "      <th>conflicting</th>\n",
              "      <th>sentiment_signal</th>\n",
              "      <th>spy_return</th>\n",
              "      <th>final_score_lag1</th>\n",
              "      <th>sentiment_signal_lag1</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2008-01-02</td>\n",
              "      <td>104.0849</td>\n",
              "      <td>-0.0911</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>-0.2732</td>\n",
              "      <td>3.0000</td>\n",
              "      <td>JPMorgan Predicts 2008 Will Be \"Nothing But Ne...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>-0.1594</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>hold</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>None</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2008-01-03</td>\n",
              "      <td>104.0347</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>1.0000</td>\n",
              "      <td>U.S. Stocks Higher After Economic Data, Monsan...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>True</td>\n",
              "      <td>False</td>\n",
              "      <td>hold</td>\n",
              "      <td>-0.0005</td>\n",
              "      <td>-0.1594</td>\n",
              "      <td>hold</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2008-01-04</td>\n",
              "      <td>101.4851</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>-0.0245</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>hold</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2008-01-07</td>\n",
              "      <td>101.3990</td>\n",
              "      <td>0.6249</td>\n",
              "      <td>0.6249</td>\n",
              "      <td>0.6249</td>\n",
              "      <td>1.0000</td>\n",
              "      <td>U.S. Stocks Climb As Hopes Increase For More F...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.9374</td>\n",
              "      <td>True</td>\n",
              "      <td>False</td>\n",
              "      <td>hold</td>\n",
              "      <td>-0.0008</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2008-01-08</td>\n",
              "      <td>99.7615</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>-0.0161</td>\n",
              "      <td>0.9374</td>\n",
              "      <td>hold</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "        date    close  compound_mean  compound_max  compound_min  \\\n",
              "0 2008-01-02 104.0849        -0.0911        0.0000       -0.2732   \n",
              "1 2008-01-03 104.0347         0.0000        0.0000        0.0000   \n",
              "2 2008-01-04 101.4851            NaN           NaN           NaN   \n",
              "3 2008-01-07 101.3990         0.6249        0.6249        0.6249   \n",
              "4 2008-01-08  99.7615            NaN           NaN           NaN   \n",
              "\n",
              "   headline_count                                           headline  \\\n",
              "0          3.0000  JPMorgan Predicts 2008 Will Be \"Nothing But Ne...   \n",
              "1          1.0000  U.S. Stocks Higher After Economic Data, Monsan...   \n",
              "2             NaN                                                NaN   \n",
              "3          1.0000  U.S. Stocks Climb As Hopes Increase For More F...   \n",
              "4             NaN                                                NaN   \n",
              "\n",
              "   compound_z  final_score too_few_headlines conflicting sentiment_signal  \\\n",
              "0         NaN      -0.1594             False       False             hold   \n",
              "1         NaN       0.0000              True       False             hold   \n",
              "2         NaN          NaN               NaN         NaN              NaN   \n",
              "3         NaN       0.9374              True       False             hold   \n",
              "4         NaN          NaN               NaN         NaN              NaN   \n",
              "\n",
              "   spy_return  final_score_lag1 sentiment_signal_lag1  \n",
              "0         NaN               NaN                  None  \n",
              "1     -0.0005           -0.1594                  hold  \n",
              "2     -0.0245            0.0000                  hold  \n",
              "3     -0.0008               NaN                   NaN  \n",
              "4     -0.0161            0.9374                  hold  "
            ]
          },
          "execution_count": 71,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "\n",
        "def align_sentiment_returns(sentiment: pd.DataFrame, prices: pd.DataFrame) -> pd.DataFrame:\n",
        "    merged = prices.merge(sentiment, on=\"date\", how=\"left\").sort_values(\"date\")\n",
        "    merged[\"spy_return\"] = merged[\"close\"].pct_change()\n",
        "\n",
        "    # shift sentiment one day forward to avoid look-ahead\n",
        "    for col in [\"final_score\", \"sentiment_signal\"]:\n",
        "        merged[f\"{col}_lag1\"] = merged[col].shift(1)\n",
        "\n",
        "    return merged\n",
        "\n",
        "\n",
        "merged_df = align_sentiment_returns(sentiment_signals, prices)\n",
        "merged_df.tail()\n",
        "merged_df.head()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "55e86255",
      "metadata": {},
      "source": [
        "## 10) Correlation and simple regression\n",
        "Evaluate whether sentiment relates to same-day or next-day SPY returns.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 78,
      "id": "48341d77",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'corr_same_day': np.float64(0.12100227722853701),\n",
              " 'corr_next_day': np.float64(0.019510885092469323),\n",
              " 'beta_same_day': [0.0002840866603005102, 0.004653361273274365],\n",
              " 't_same_day': [1.3928142387086206, 6.906355625665744],\n",
              " 'beta_next_day': [0.0004560114551580251, 0.0007585992773132331],\n",
              " 't_next_day': [2.2185340766833965, 1.1056359859898877]}"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "\n",
        "from math import sqrt\n",
        "import numpy as np\n",
        "\n",
        "def simple_regression(y, x):\n",
        "    # Adds intercept; returns beta and t-stat\n",
        "    X = np.column_stack([np.ones(len(x)), x])\n",
        "    beta, *_ = np.linalg.lstsq(X, y, rcond=None)\n",
        "    y_hat = X @ beta\n",
        "    resid = y - y_hat\n",
        "    dof = max(len(x) - X.shape[1], 1)\n",
        "    sigma2 = (resid @ resid) / dof\n",
        "    cov_beta = sigma2 * np.linalg.inv(X.T @ X)\n",
        "    se = np.sqrt(np.diag(cov_beta))\n",
        "    t_stats = beta / se\n",
        "    return beta, t_stats\n",
        "\n",
        "# drop NaNs for analysis\n",
        "analysis_df = merged_df.dropna(subset=[\"spy_return\", \"final_score\", \"final_score_lag1\"])\n",
        "\n",
        "# correlations\n",
        "corr_same = analysis_df[[\"spy_return\", \"final_score\"]].corr().iloc[0,1]\n",
        "corr_next = analysis_df[[\"spy_return\", \"final_score_lag1\"]].corr().iloc[0,1]\n",
        "\n",
        "beta_same, t_same = simple_regression(\n",
        "    analysis_df[\"spy_return\"].values,\n",
        "    analysis_df[\"final_score\"].values,\n",
        ")\n",
        "\n",
        "beta_next, t_next = simple_regression(\n",
        "    analysis_df[\"spy_return\"].values,\n",
        "    analysis_df[\"final_score_lag1\"].values,\n",
        ")\n",
        "\n",
        "display({\n",
        "    \"corr_same_day\": corr_same,\n",
        "    \"corr_next_day\": corr_next,\n",
        "    \"beta_same_day\": beta_same.tolist(),\n",
        "    \"t_same_day\": t_same.tolist(),\n",
        "    \"beta_next_day\": beta_next.tolist(),\n",
        "    \"t_next_day\": t_next.tolist(),\n",
        "})\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "631e9aa1",
      "metadata": {},
      "source": [
        "## 11) Returns by sentiment bucket\n",
        "Compare average SPY returns when prior-day sentiment is positive vs. negative.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 73,
      "id": "772a1dca",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                    mean    std  count\n",
            "sentiment_bucket                      \n",
            "bearish          -0.0003 0.0146    605\n",
            "neutral           0.0007 0.0116   1932\n",
            "bullish           0.0005 0.0100    970\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/var/folders/hp/kfy3hmy15mn6824slvj7zgd40000gn/T/ipykernel_64658/727636975.py:9: FutureWarning: The default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n",
            "  .groupby(\"sentiment_bucket\")[\"spy_return\"]\n"
          ]
        }
      ],
      "source": [
        "\n",
        "merged_df[\"sentiment_bucket\"] = pd.cut(\n",
        "    merged_df[\"final_score_lag1\"],\n",
        "    bins=[-np.inf, -0.2, 0.2, np.inf],\n",
        "    labels=[\"bearish\", \"neutral\", \"bullish\"],\n",
        ")\n",
        "\n",
        "bucket_stats = (\n",
        "    merged_df.dropna(subset=[\"spy_return\", \"sentiment_bucket\"])\n",
        "    .groupby(\"sentiment_bucket\")[\"spy_return\"]\n",
        "    .agg([\"mean\", \"std\", \"count\"])\n",
        ")\n",
        "print(bucket_stats)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "eb04db79",
      "metadata": {},
      "source": [
        "## 12) Export merged dataset for teammates\n",
        "Provides aligned sentiment + SPY returns for downstream strategy work.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "23779667",
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "merged_path = Path(\"models/sentiment_model/sentiment_with_spy.csv\")\n",
        "merged_df.to_csv(merged_path, index=False)\n",
        "print(f\"Saved merged sentiment/price data to {merged_path}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5c48c2a3",
      "metadata": {},
      "source": [
        "## 13) Coverage & distribution checks\n",
        "Quick summary of headline coverage and distribution of sentiment buckets (uses merged_df).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 76,
      "id": "d37e5e45",
      "metadata": {},
      "outputs": [
        {
          "ename": "SyntaxError",
          "evalue": "unterminated string literal (detected at line 9) (4102714778.py, line 9)",
          "output_type": "error",
          "traceback": [
            "  \u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[76]\u001b[39m\u001b[32m, line 9\u001b[39m\n\u001b[31m    \u001b[39m\u001b[31mprint(\"Sentiment bucket counts:)\u001b[39m\n          ^\n\u001b[31mSyntaxError\u001b[39m\u001b[31m:\u001b[39m unterminated string literal (detected at line 9)\n"
          ]
        }
      ],
      "source": [
        "\n",
        "coverage = {\n",
        "    \"date_range\": (merged_df[\"date\"].min(), merged_df[\"date\"].max()),\n",
        "    \"headline_days\": merged_df[\"date\"].nunique(),\n",
        "    \"avg_headlines_per_day\": merged_df[\"headline_count\"].mean(),\n",
        "}\n",
        "print(coverage)\n",
        "\n",
        "bucket_counts = merged_df[\"sentiment_bucket\"].value_counts(dropna=False)\n",
        "print(\"Sentiment bucket counts:)\n",
        "print(bucket_counts)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d463737b",
      "metadata": {},
      "source": [
        "## 14) SPY return for a given day + that day's headlines + sentiment scores"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 84,
      "id": "e155de59",
      "metadata": {},
      "outputs": [],
      "source": [
        "def inspect_day(target_date, prices_df, headline_df, sentiment_df):\n",
        "    target_date = pd.to_datetime(target_date).normalize()\n",
        "\n",
        "    # Need both this day and previous day\n",
        "    px_today = prices_df[prices_df[\"date\"] == target_date]\n",
        "    px_prev  = prices_df[prices_df[\"date\"] == target_date - pd.Timedelta(days=1)]\n",
        "\n",
        "    if px_today.empty:\n",
        "        print(f\"No SPY price data for {target_date.date()}\")\n",
        "        return\n",
        "\n",
        "    if px_prev.empty:\n",
        "        ret = np.nan  # no previous price\n",
        "    else:\n",
        "        ret = (px_today[\"close\"].iloc[0] / px_prev[\"close\"].iloc[0]) - 1\n",
        "\n",
        "    # === SENTIMENT ===\n",
        "    sent_row = sentiment_df[sentiment_df[\"date\"] == target_date]\n",
        "    if sent_row.empty:\n",
        "        print(f\"No sentiment data for {target_date.date()}\")\n",
        "        return\n",
        "    sent_row = sent_row.iloc[0]\n",
        "\n",
        "    # === SHOW HEADLINES ===\n",
        "    day_headlines = headline_df[headline_df[\"date\"] == target_date].copy()\n",
        "    if not day_headlines.empty:\n",
        "        print(\"---- \ud83d\udcf0 Headlines & Compound Scores ----\")\n",
        "        display(day_headlines[[\"headline\", \"compound\"]])\n",
        "\n",
        "\n",
        "def build_daily_summary(prices, sentiment, headlines):\n",
        "    # compute daily returns first\n",
        "    prices = prices.sort_values(\"date\").copy()\n",
        "    prices[\"spy_return\"] = prices[\"close\"].pct_change()\n",
        "\n",
        "    # group headlines into a single string per day\n",
        "    grouped_headlines = (\n",
        "        headlines.groupby(\"date\")[\"headline\"]\n",
        "        .apply(lambda s: \" || \".join(s.astype(str)))\n",
        "        .reset_index()\n",
        "        .rename(columns={\"headline\": \"headlines\"})\n",
        "    )\n",
        "\n",
        "    # merge everything\n",
        "    merged = prices.merge(sentiment, on=\"date\", how=\"left\")\\\n",
        "                   .merge(grouped_headlines, on=\"date\", how=\"left\")\n",
        "\n",
        "    return merged.sort_values(\"date\")\n",
        "\n",
        "daily_summary = build_daily_summary(prices, sentiment_signals, scored_headlines)\n",
        "daily_summary.head()\n",
        "daily_summary.to_csv('daily_summary.csv', index=False)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b3ac5ccb",
      "metadata": {},
      "source": [
        "## 15) Quick takeaways (manual)\n",
        "Use the outputs above to describe whether sentiment has any predictive power on SPY returns and how coverage limitations might affect reliability.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 16) Weekly / Fortnightly / Monthly aggregation\n",
        "Aggregate sentiment to coarser horizons (weighted by headline counts) for higher-level validation.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "from typing import Dict\n",
        "\n",
        "def aggregate_period(sent_df: pd.DataFrame, freq: str) -> pd.DataFrame:\n",
        "    df = sent_df.copy()\n",
        "    df = df.sort_values(\"date\").set_index(\"date\")\n",
        "\n",
        "    def agg_block(sub: pd.DataFrame) -> pd.Series:\n",
        "        total = sub[\"headline_count\"].sum()\n",
        "        def wavg(col):\n",
        "            return (sub[col] * sub[\"headline_count\"]).sum() / total if total > 0 else np.nan\n",
        "        return pd.Series(\n",
        "            {\n",
        "                \"compound_mean_wt\": wavg(\"compound_mean\"),\n",
        "                \"final_score_wt\": wavg(\"final_score\"),\n",
        "                \"compound_max\": sub[\"compound_max\"].max(),\n",
        "                \"compound_min\": sub[\"compound_min\"].min(),\n",
        "                \"headline_count\": total,\n",
        "                \"start_date\": sub.index.min(),\n",
        "                \"end_date\": sub.index.max(),\n",
        "            }\n",
        "        )\n",
        "\n",
        "    grouped = df.resample(freq).apply(agg_block)\n",
        "    grouped = grouped.reset_index().rename(columns={\"date\": \"period_end\"})\n",
        "    return grouped\n",
        "\n",
        "freq_map = {\n",
        "    \"W-FRI\": \"weekly\",\n",
        "    \"2W-FRI\": \"fortnightly\",\n",
        "    \"M\": \"monthly\",\n",
        "}\n",
        "\n",
        "aggregated_periods: Dict[str, pd.DataFrame] = {}\n",
        "for freq, label in freq_map.items():\n",
        "    aggregated_periods[label] = aggregate_period(sentiment_signals, freq)\n",
        "\n",
        "aggregated_periods[\"weekly\"].head()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 17) Period returns & correlations\n",
        "Align period sentiment with SPY period returns and check correlations/regressions using lagged sentiment (no look-ahead).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "    def aggregate_returns(price_df: pd.DataFrame, freq: str) -> pd.DataFrame:\n",
        "        p = price_df.sort_values(\"date\").set_index(\"date\")[\"close\"]\n",
        "        grp = p.resample(freq).agg([\"first\", \"last\"])\n",
        "        grp[\"period_return\"] = grp[\"last\"] / grp[\"first\"] - 1\n",
        "        return grp.reset_index().rename(columns={\"date\": \"period_end\"})\n",
        "\n",
        "    def corr_reg_for_period(sent_period: pd.DataFrame, returns_period: pd.DataFrame, label: str):\n",
        "        merged = returns_period.merge(sent_period, on=\"period_end\", how=\"inner\").sort_values(\"period_end\")\n",
        "        merged[\"final_score_wt_lag1\"] = merged[\"final_score_wt\"].shift(1)\n",
        "\n",
        "        df = merged.dropna(subset=[\"period_return\", \"final_score_wt\", \"final_score_wt_lag1\"])\n",
        "        corr_same = df[[\"period_return\", \"final_score_wt\"]].corr().iloc[0,1]\n",
        "        corr_next = df[[\"period_return\", \"final_score_wt_lag1\"]].corr().iloc[0,1]\n",
        "\n",
        "        beta_same, t_same = simple_regression(df[\"period_return\"].values, df[\"final_score_wt\"].values)\n",
        "        beta_next, t_next = simple_regression(df[\"period_return\"].values, df[\"final_score_wt_lag1\"].values)\n",
        "\n",
        "        print(f\"\n",
        "== {label.capitalize()} ==\")\n",
        "        print({\n",
        "            \"rows\": len(df),\n",
        "            \"corr_same_period\": corr_same,\n",
        "            \"corr_next_period\": corr_next,\n",
        "            \"beta_same\": beta_same.tolist(),\n",
        "            \"t_same\": t_same.tolist(),\n",
        "            \"beta_next\": beta_next.tolist(),\n",
        "            \"t_next\": t_next.tolist(),\n",
        "        })\n",
        "        return merged\n",
        "\n",
        "    aggregated_returns: Dict[str, pd.DataFrame] = {}\n",
        "    merged_period_outputs: Dict[str, pd.DataFrame] = {}\n",
        "\n",
        "    for freq, label in freq_map.items():\n",
        "        rets = aggregate_returns(prices, freq)\n",
        "        aggregated_returns[label] = rets\n",
        "        merged_period_outputs[label] = corr_reg_for_period(\n",
        "            aggregated_periods[label],\n",
        "            rets,\n",
        "            label,\n",
        "        )\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 18) Export aggregated datasets\n",
        "Save period-level sentiment + returns for teammates.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "for label, df_out in merged_period_outputs.items():\n",
        "    out_path = Path(f\"models/sentiment_model/sentiment_with_spy_{label}.csv\")\n",
        "    df_out.to_csv(out_path, index=False)\n",
        "    print(f\"Saved {label} sentiment/price data to {out_path}\")\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}